{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Write a python program to display all the header tags from ‘en.wikipedia.org/wiki/Main_Page’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki(URL):\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    titles = soup.find_all('span', class_= \"mw-headline\")\n",
    "    headings = [] #declaring an empty list\n",
    "    for i in titles:\n",
    "        headings.append(i.text)\n",
    "    print(headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki(\"https://en.wikipedia.org/wiki/Main_Page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working for arriving at the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://en.wikipedia.org/wiki/Main_Page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_title = soup.find('span', class_= \"mw-headline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = soup.find_all('span', class_= \"mw-headline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headings = [] #declaring an empty list\n",
    "for i in titles:\n",
    "    headings.append(i.text)\n",
    "headings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. Name, IMDB rating, Year of release) and save it in form of a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb(URL):\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    titles = soup.find_all('td', class_= \"titleColumn\")\n",
    "    headings = [] #declaring an empty list\n",
    "    for i in titles:\n",
    "        j = i.text.replace(\"\\n      \",\"\")\n",
    "        j = j.replace(\"\\n\",\"\")\n",
    "        headings.append(j)\n",
    "    return headings[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb(\"https://www.imdb.com/chart/top/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working for arriving at the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://www.imdb.com/chart/top/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_title = soup.find('td', class_= \"titleColumn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = soup.find_all('td', class_= \"titleColumn\")\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headings = [] #declaring an empty list\n",
    "for i in titles:\n",
    "    j = i.text.replace(\"\\n      \",\"\")\n",
    "    j = j.replace(\"\\n\",\"\")\n",
    "    headings.append(j)\n",
    "headings[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. Name, IMDB rating, Year of release) and save it in form of a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb(URL):\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    titles = soup.find_all('td', class_= \"titleColumn\")\n",
    "    headings = [] #declaring an empty list\n",
    "    for i in titles:\n",
    "        j = i.text.replace(\"\\n      \",\"\")\n",
    "        j = j.replace(\"\\n\",\"\")\n",
    "        headings.append(j)\n",
    "    return headings[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb(\"https://www.imdb.com/india/top-rated-indian-movies/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working for arriving at the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://www.imdb.com/india/top-rated-indian-movies/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_title = soup.find('td', class_= \"titleColumn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = soup.find_all('td', class_= \"titleColumn\")\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headings = [] #declaring an empty list\n",
    "for i in titles:\n",
    "    j = i.text.replace(\"\\n      \",\"\")\n",
    "    j = j.replace(\"\\n\",\"\")\n",
    "    headings.append(j)\n",
    "headings[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write a python program to scrap book name, author name, genre and book review of any 5 books from ‘www.bookpage.com’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def books(URL):\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    titles = soup.find_all('div', class_= \"flex-article-content\")\n",
    "    headings = [] #declaring an empty list\n",
    "    for i in titles:\n",
    "        j = i.text.replace(\"\\n      \",\"\")\n",
    "        j = j.replace(\"\\n\",\"\")\n",
    "        headings.append(j)\n",
    "    return headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books(\"https://bookpage.com/reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working for arriving at the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://bookpage.com/reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_title = soup.find('div', class_= \"flex-article-content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = soup.find_all('div', class_= \"flex-article-content\")\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headings = [] #declaring an empty list\n",
    "for i in titles:\n",
    "    j = i.text.replace(\"\\n      \",\"\")\n",
    "    j = j.replace(\"\\n\",\"\")\n",
    "    headings.append(j)\n",
    "headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. You have to scrape:\n",
    "i) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "ii) Top 10 ODI Batsmen in men along with the records of their team and rating.\n",
    "iii) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cricket(URL):\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "    gdp = soup.find_all(\"table\", attrs={\"class\": \"table\"})\n",
    "    table1 = gdp[0]\n",
    "    # the head will form our column names\n",
    "    body = table1.find_all(\"tr\")\n",
    "    # Head values (Column names) are the first items of the body list\n",
    "    head = body[0] # 0th item is the header row\n",
    "    body_rows = body[1:] # All other items becomes the rest of the rows\n",
    "\n",
    "    # Lets now iterate through the head HTML code and make list of clean headings\n",
    "\n",
    "    # Declare empty list to keep Columns names\n",
    "    headings = []\n",
    "    for item in head.find_all(\"th\"): # loop through all th elements\n",
    "        # convert the th elements to text and strip \"\\n\"\n",
    "        item = (item.text).rstrip(\"\\n\")\n",
    "        # append the clean column name to headings\n",
    "        headings.append(item)\n",
    "    \n",
    "    # Next is now to loop though the rest of the rows\n",
    "\n",
    "    #print(body_rows[0])\n",
    "    all_rows = [] # will be a list for list for all rows\n",
    "    for row_num in range(len(body_rows)): # A row at a time\n",
    "        row = [] # this will old entries for one row\n",
    "        for row_item in body_rows[row_num].find_all(\"td\"): #loop through all row entries\n",
    "            # row_item.text removes the tags from the entries\n",
    "            # the following regex is to remove \\xa0 and \\n and comma from row_item.text\n",
    "            # xa0 encodes the flag, \\n is the newline and comma separates thousands in numbers\n",
    "            aa = re.sub(\"(\\xa0)|(\\n)|,\",\"\",row_item.text)\n",
    "            #append aa to row - note one row entry is being appended\n",
    "            row.append(aa)\n",
    "        # append one row to all_rows\n",
    "        all_rows.append(row)\n",
    "        \n",
    "    # We can now use the data on all_rowsa and headings to make a table\n",
    "    # all_rows becomes our data and headings the column names\n",
    "    df = pd.DataFrame(data=all_rows,columns=headings)\n",
    "    return df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cricket(\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cricket(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cricket(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working for arriving at the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page = requests.get(\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\")\n",
    "#page = requests.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\")\n",
    "page = requests.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, \"lxml\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line will generate a list of HTML content for each table\n",
    "gdp = soup.find_all(\"table\", attrs={\"class\": \"table\"})\n",
    "print(\"Number of tables on site: \",len(gdp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = gdp[0]\n",
    "# the head will form our column names\n",
    "body = table1.find_all(\"tr\")\n",
    "# Head values (Column names) are the first items of the body list\n",
    "head = body[0] # 0th item is the header row\n",
    "body_rows = body[1:] # All other items becomes the rest of the rows\n",
    "\n",
    "# Lets now iterate through the head HTML code and make list of clean headings\n",
    "\n",
    "# Declare empty list to keep Columns names\n",
    "headings = []\n",
    "for item in head.find_all(\"th\"): # loop through all th elements\n",
    "    # convert the th elements to text and strip \"\\n\"\n",
    "    item = (item.text).rstrip(\"\\n\")\n",
    "    # append the clean column name to headings\n",
    "    headings.append(item)\n",
    "print(headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next is now to loop though the rest of the rows\n",
    "\n",
    "#print(body_rows[0])\n",
    "all_rows = [] # will be a list for list for all rows\n",
    "for row_num in range(len(body_rows)): # A row at a time\n",
    "    row = [] # this will old entries for one row\n",
    "    for row_item in body_rows[row_num].find_all(\"td\"): #loop through all row entries\n",
    "        # row_item.text removes the tags from the entries\n",
    "        # the following regex is to remove \\xa0 and \\n and comma from row_item.text\n",
    "        # xa0 encodes the flag, \\n is the newline and comma separates thousands in numbers\n",
    "        aa = re.sub(\"(\\xa0)|(\\n)|,\",\"\",row_item.text)\n",
    "        #append aa to row - note one row entry is being appended\n",
    "        row.append(aa)\n",
    "    # append one row to all_rows\n",
    "    all_rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now use the data on all_rowsa and headings to make a table\n",
    "# all_rows becomes our data and headings the column names\n",
    "df = pd.DataFrame(data=all_rows,columns=headings)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. You have to scrape:\n",
    "i) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "ii) Top 10 women’s ODI players along with the records of their team and rating.\n",
    "iii) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cricket(URL):\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "    gdp = soup.find_all(\"table\", attrs={\"class\": \"table\"})\n",
    "    table1 = gdp[0]\n",
    "    # the head will form our column names\n",
    "    body = table1.find_all(\"tr\")\n",
    "    # Head values (Column names) are the first items of the body list\n",
    "    head = body[0] # 0th item is the header row\n",
    "    body_rows = body[1:] # All other items becomes the rest of the rows\n",
    "\n",
    "    # Lets now iterate through the head HTML code and make list of clean headings\n",
    "\n",
    "    # Declare empty list to keep Columns names\n",
    "    headings = []\n",
    "    for item in head.find_all(\"th\"): # loop through all th elements\n",
    "        # convert the th elements to text and strip \"\\n\"\n",
    "        item = (item.text).rstrip(\"\\n\")\n",
    "        # append the clean column name to headings\n",
    "        headings.append(item)\n",
    "    \n",
    "    # Next is now to loop though the rest of the rows\n",
    "\n",
    "    #print(body_rows[0])\n",
    "    all_rows = [] # will be a list for list for all rows\n",
    "    for row_num in range(len(body_rows)): # A row at a time\n",
    "        row = [] # this will old entries for one row\n",
    "        for row_item in body_rows[row_num].find_all(\"td\"): #loop through all row entries\n",
    "            # row_item.text removes the tags from the entries\n",
    "            # the following regex is to remove \\xa0 and \\n and comma from row_item.text\n",
    "            # xa0 encodes the flag, \\n is the newline and comma separates thousands in numbers\n",
    "            aa = re.sub(\"(\\xa0)|(\\n)|,\",\"\",row_item.text)\n",
    "            #append aa to row - note one row entry is being appended\n",
    "            row.append(aa)\n",
    "        # append one row to all_rows\n",
    "        all_rows.append(row)\n",
    "        \n",
    "    # We can now use the data on all_rowsa and headings to make a table\n",
    "    # all_rows becomes our data and headings the column names\n",
    "    df = pd.DataFrame(data=all_rows,columns=headings)\n",
    "    return df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cricket(\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cricket(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cricket(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working for arriving at the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page = requests.get(\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\")\n",
    "#page = requests.get(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\")\n",
    "page = requests.get(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, \"lxml\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line will generate a list of HTML content for each table\n",
    "gdp = soup.find_all(\"table\", attrs={\"class\": \"table\"})\n",
    "print(\"Number of tables on site: \",len(gdp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = gdp[0]\n",
    "# the head will form our column names\n",
    "body = table1.find_all(\"tr\")\n",
    "# Head values (Column names) are the first items of the body list\n",
    "head = body[0] # 0th item is the header row\n",
    "body_rows = body[1:] # All other items becomes the rest of the rows\n",
    "\n",
    "# Lets now iterate through the head HTML code and make list of clean headings\n",
    "\n",
    "# Declare empty list to keep Columns names\n",
    "headings = []\n",
    "for item in head.find_all(\"th\"): # loop through all th elements\n",
    "    # convert the th elements to text and strip \"\\n\"\n",
    "    item = (item.text).rstrip(\"\\n\")\n",
    "    # append the clean column name to headings\n",
    "    headings.append(item)\n",
    "print(headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next is now to loop though the rest of the rows\n",
    "\n",
    "#print(body_rows[0])\n",
    "all_rows = [] # will be a list for list for all rows\n",
    "for row_num in range(len(body_rows)): # A row at a time\n",
    "    row = [] # this will old entries for one row\n",
    "    for row_item in body_rows[row_num].find_all(\"td\"): #loop through all row entries\n",
    "        # row_item.text removes the tags from the entries\n",
    "        # the following regex is to remove \\xa0 and \\n and comma from row_item.text\n",
    "        # xa0 encodes the flag, \\n is the newline and comma separates thousands in numbers\n",
    "        aa = re.sub(\"(\\xa0)|(\\n)|,\",\"\",row_item.text)\n",
    "        #append aa to row - note one row entry is being appended\n",
    "        row.append(aa)\n",
    "    # append one row to all_rows\n",
    "    all_rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now use the data on all_rowsa and headings to make a table\n",
    "# all_rows becomes our data and headings the column names\n",
    "df = pd.DataFrame(data=all_rows,columns=headings)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Write a python program to scrape details of all the mobile phones under Rs. 20,000 listed on Amazon.in. The scraped data should include Product Name, Price, Image URL and Average Rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amazon(URL):\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "    images = soup.find_all(\"img\")\n",
    "    img = []\n",
    "    for image in images:\n",
    "        img.append(image['src'])\n",
    "    df = DataFrame(img[3:13], columns = ['Image'])\n",
    "    titles = soup.find_all('span', class_= \"a-size-medium a-color-base a-text-normal\")\n",
    "    model = []\n",
    "    for i in titles:\n",
    "        j = i.text.replace(\"\\n\",\"\")\n",
    "        model.append(j)\n",
    "    len(model)\n",
    "    df['model'] = model[0:10]\n",
    "    titles = soup.find_all('span', class_= \"a-price-whole\")\n",
    "    price = []\n",
    "    for i in titles:\n",
    "        j = i.text.replace(\"\\n\",\"\")\n",
    "        price.append(j)\n",
    "    df['price'] = price[0:10]\n",
    "    titles = soup.find_all('a', class_=\"a-popover-trigger a-declarative\")\n",
    "    ratings = []\n",
    "    for i in titles:\n",
    "        j = i.text.replace(\"\\n\",\"\")\n",
    "        ratings.append(j)\n",
    "    df['ratings'] = ratings[0:10]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon(\"https://www.amazon.in/s?k=mobile+under+20000&ref=nb_sb_noss_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working for arriving at the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://www.amazon.in/s?k=mobile+under+20000&ref=nb_sb_noss_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, \"lxml\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = soup.find_all(\"img\")\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = []\n",
    "for image in images:\n",
    "    img.append(image['src'])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(img[3:13], columns = ['Image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = soup.find_all('span', class_= \"a-size-medium a-color-base a-text-normal\")\n",
    "model = []\n",
    "for i in titles:\n",
    "    j = i.text.replace(\"\\n\",\"\")\n",
    "    model.append(j)\n",
    "len(model)\n",
    "df['model'] = model[0:10]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = soup.find_all('span', class_= \"a-price-whole\")\n",
    "price = []\n",
    "for i in titles:\n",
    "    j = i.text.replace(\"\\n\",\"\")\n",
    "    price.append(j)\n",
    "df['price'] = price[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = soup.find_all('a', class_=\"a-popover-trigger a-declarative\")\n",
    "ratings = []\n",
    "for i in titles:\n",
    "    j = i.text.replace(\"\\n\",\"\")\n",
    "    ratings.append(j)\n",
    "df['ratings'] = ratings[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Write a python program to extract information about the local weather from the National Weather Service website of USA, https://www.weather.gov/ for the city, San Francisco. You need to extract data about 7 day extended forecast display for the city. The data should include period, short description, temperature and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather(URL):\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "    titles = soup.find_all('li', class_= \"forecast-tombstone\")\n",
    "    headings1 = []\n",
    "    for i in titles:\n",
    "        j = i.text.replace(\"\\n\",\"\")\n",
    "        headings1.append(j)\n",
    "    from pandas import DataFrame\n",
    "    df = DataFrame(headings1, columns = ['Day'])\n",
    "    df[['Day','Temperature']] = df.Day.str.split(\": \",expand=True)\n",
    "    titles2 = soup.find_all('div', class_= \"row row-odd row-forecast\")\n",
    "    headings2 = []\n",
    "    for i in titles2:\n",
    "        j = i.text.replace(\"\\n\",\"\")\n",
    "        headings2.append(j)\n",
    "    titles3 = soup.find_all('div', class_= \"row row-even row-forecast\")\n",
    "    headings3 = []\n",
    "    for i in titles3:\n",
    "        j = i.text.replace(\"\\n\",\"\")\n",
    "        headings3.append(j)\n",
    "    forecast = []\n",
    "    x = 0\n",
    "    y = 0\n",
    "    for i in range(0,len(df[\"Day\"])):\n",
    "        if(i%2==0):\n",
    "            forecast.append(headings3[x])\n",
    "            x = x + 1\n",
    "        else:\n",
    "            forecast.append(headings2[y])\n",
    "            y = y + 1 \n",
    "    df[\"Forecast\"] = forecast\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather(\"https://forecast.weather.gov/MapClick.php?lat=37.7771&lon=-122.4196#.YJaGybVLjIU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working for arriving at the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://forecast.weather.gov/MapClick.php?lat=37.7771&lon=-122.4196#.YJaGybVLjIU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, \"lxml\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_title = soup.find('li', class_= \"forecast-tombstone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = soup.find_all('li', class_= \"forecast-tombstone\")\n",
    "headings1 = []\n",
    "for i in titles:\n",
    "    j = i.text.replace(\"\\n\",\"\")\n",
    "    headings1.append(j)\n",
    "headings1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(headings1, columns = ['Day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Day','Temperature']] = df.Day.str.split(\": \",expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_title = soup.find('div', class_= \"row row-odd row-forecast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles2 = soup.find_all('div', class_= \"row row-odd row-forecast\")\n",
    "headings2 = []\n",
    "for i in titles2:\n",
    "    j = i.text.replace(\"\\n\",\"\")\n",
    "    headings2.append(j)\n",
    "headings2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles3 = soup.find_all('div', class_= \"row row-even row-forecast\")\n",
    "headings3 = []\n",
    "for i in titles3:\n",
    "    j = i.text.replace(\"\\n\",\"\")\n",
    "    headings3.append(j)\n",
    "headings3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headings3[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = []\n",
    "x = 0\n",
    "y = 0\n",
    "for i in range(0,len(df[\"Day\"])):\n",
    "    if(i%2==0):\n",
    "        forecast.append(headings3[x])\n",
    "        x = x + 1\n",
    "    else:\n",
    "        forecast.append(headings2[y])\n",
    "        y = y + 1 \n",
    "forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Forecast\"] = forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

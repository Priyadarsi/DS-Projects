{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement:\n",
    "Avocado is a fruit consumed by people heavily in the United States. \n",
    "\n",
    "Content\n",
    "This data was downloaded from the Hass Avocado Board website in May of 2018 & compiled into a single CSV. \n",
    "\n",
    "The table below represents weekly 2018 retail scan data for National retail volume (units) and price. Retail scan data comes directly from retailers’ cash registers based on actual retail sales of Hass avocados. \n",
    "\n",
    "Starting in 2013, the table below reflects an expanded, multi-outlet retail data set. Multi-outlet reporting includes an aggregation of the following channels: grocery, mass, club, drug, dollar and military. The Average Price (of avocados) in the table reflects a per unit (per avocado) cost, even when multiple units (avocados) are sold in bags. \n",
    "\n",
    "The Product Lookup codes (PLU’s) in the table are only for Hass avocados. Other varieties of avocados (e.g. greenskins) are not included in this table.\n",
    "\n",
    "Some relevant columns in the dataset:\n",
    "\n",
    "Date - The date of the observation\n",
    "AveragePrice - the average price of a single avocado\n",
    "type - conventional or organic\n",
    "year - the year\n",
    "Region - the city or region of the observation\n",
    "Total Volume - Total number of avocados sold\n",
    "4046 - Total number of avocados with PLU 4046 sold\n",
    "4225 - Total number of avocados with PLU 4225 sold\n",
    "4770 - Total number of avocados with PLU 4770 sold\n",
    "\n",
    "\n",
    "Inspiration /Label \n",
    "\n",
    "Your task is to make a mode that can consider the data provided and predict the Average Price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import confusion_matrix, classification_report \n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import zscore #to remove outliers\n",
    "from scipy.stats import skew\n",
    "import requests\n",
    "import pandas_profiling\n",
    "import io\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"avocado.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape # check the data dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the dataset has only 1518 rows but the file is being imported with 16468 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Date\"][1516] # we need to keep the dataframe limited to 1516 rows and also drop the extra index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='Unnamed: 0', axis=1,inplace = True)#column is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Date\"][16467]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 16467\n",
    "while j>1516:\n",
    "    df = df.drop(index = j)\n",
    "    j -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df #additional rows are dropped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns # check the column names for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## handle categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical columns in the dataset are as follows:\n",
    "#date, type, region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting date column to 3 different columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"day\", \"month\", \"year\"]] = df[\"Date\"].str.split(\"-\", expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"Date\", axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "LE=LabelEncoder()\n",
    "df[\"type\"] = LE.fit_transform(df[\"type\"])\n",
    "df[\"region\"] = LE.fit_transform(df[\"region\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#day, month and year columns are still object types, converting them to float...\n",
    "df['day'] = pd.to_numeric(df['day'], downcast='float')\n",
    "df['month'] = pd.to_numeric(df['month'], downcast='float')\n",
    "df['year'] = pd.to_numeric(df['year'], downcast='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## handle null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isna().sum() #check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(df.isnull())\n",
    "plt.title(\"Null Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#No null values to be handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='4046', y='AveragePrice', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='4225', y='AveragePrice', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='4770', y='AveragePrice', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking the distribution of values of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in df:\n",
    "    print(col)\n",
    "    \n",
    "    plt.figure()\n",
    "    sns.kdeplot(df[col], shade = True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Checking the distribution of values of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in df:\n",
    "    print(col)\n",
    "    \n",
    "    plt.figure()\n",
    "    sns.countplot(df[col])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pre_profile = df.profile_report(title=\"avocado\")\n",
    "#pre_profile.to_file(output_file=\"avocado.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pre_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#type has constant value \"0\"\tConstant\n",
    "#Total Volume is highly correlated with 4046 and 5 other fields\tHigh correlation\n",
    "#4046 is highly correlated with Total Volume and 3 other fields\tHigh correlation\n",
    "#4225 is highly correlated with Total Volume and 5 other fields\tHigh correlation\n",
    "#4770 is highly correlated with Total Volume and 3 other fields\tHigh correlation\n",
    "#Total Bags is highly correlated with Total Volume\tHigh correlation\n",
    "#Small Bags is highly correlated with Total Volume\tHigh correlation\n",
    "#Large Bags is highly correlated with Total Volume\tHigh correlation\n",
    "#type is highly correlated with year\tHigh correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remove outliers before skewness check and before x, y split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.boxplot(figsize=[20,8])\n",
    "plt.subplots_adjust(bottom=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Removing outliers by z score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "z = np.abs(zscore(df))\n",
    "new_df = df[(z<3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataloss = ((1517-0)/1517)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Outliers are not removed since data loss is very high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check co-relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[22,12])\n",
    "cor = df.corr()\n",
    "sns.heatmap(cor, annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Arrange co-relation in descending order. Dropping columns should be the last option to prevent data loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cor[\"AveragePrice\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = ['AveragePrice', 'Total Volume', '4046', '4225', '4770', 'Total Bags',\n",
    "       'Small Bags', 'Large Bags', 'XLarge Bags', 'type', 'year', 'region',\n",
    "       'day', 'month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df[columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check for skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = df.drop('AveragePrice',axis=1)\n",
    "y = df['AveragePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in df:\n",
    "    print(col)\n",
    "    print(skew(df[col]))\n",
    "    \n",
    "    plt.figure()\n",
    "    sns.distplot(df[col])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x.skew() # check skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import power_transform\n",
    "df_new = power_transform(x)\n",
    "\n",
    "df_new = pd.DataFrame(df_new, columns = x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_new.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit(x_train,y_train)\n",
    "print('Coefficients: \\n', lm.coef_)\n",
    "predictions = lm.predict(x_test)\n",
    "plt.scatter(y_test,predictions)\n",
    "plt.xlabel('Y Test')\n",
    "plt.ylabel('Predicted Y')\n",
    "\n",
    "# calculate these metrics by hand!\n",
    "from sklearn import metrics\n",
    "\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, predictions))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, predictions))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
    "print('Variance:',metrics.explained_variance_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha values could be .00001, .0001, 0.01,.1,1,10, higher values reduce all coefficients towards 0 and impact output\n",
    "# default value of alpha is 0.01\n",
    "ls = Lasso(alpha=0.00001)\n",
    "ls.fit(x_train, y_train)\n",
    "ls.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd=Ridge(alpha=0.00001)\n",
    "rd.fit(x_train, y_train)\n",
    "rd.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ElasticNet\n",
    "from sklearn.linear_model import ElasticNet\n",
    "enr = ElasticNet(alpha= 0.00001)\n",
    "enr.fit(x_train, y_train)\n",
    "enr.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "# Initialize CatBoostRegressor\n",
    "model = CatBoostRegressor(iterations=10,learning_rate=0.5,depth=2)\n",
    "# Fit model\n",
    "model.fit(x_train,y_train)\n",
    "# Get predictions\n",
    "preds = model.predict(x_test)\n",
    "\n",
    "plt.scatter(y_test,preds)\n",
    "plt.xlabel('Y Test')\n",
    "plt.ylabel('Predicted Y')\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, preds))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, preds))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, preds)))\n",
    "print('R2 Score', r2_score(y_test, preds))\n",
    "print('Variance:',metrics.explained_variance_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#decisiontreeregressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fit regression model\n",
    "regr_1 = DecisionTreeRegressor(max_depth=5)\n",
    "regr_1.fit(x_train,y_train)\n",
    "\n",
    "# Predict\n",
    "preds = regr_1.predict(x_test)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(y_test,preds)\n",
    "plt.xlabel('Y Test')\n",
    "plt.ylabel('Predicted Y')\n",
    "\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, preds))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, preds))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, preds)))\n",
    "print('Variance:',metrics.explained_variance_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "gs = GridSearchCV(model,\n",
    "                  param_grid = {'max_depth': range(1, 11),\n",
    "                                'min_samples_split': range(10, 60, 10)},\n",
    "                  cv=5,\n",
    "                  n_jobs=1,\n",
    "                  scoring='neg_mean_squared_error')\n",
    "\n",
    "gs.fit(x_train, y_train)\n",
    "\n",
    "print(gs.best_params_)\n",
    "print(-gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_model = DecisionTreeRegressor(max_depth=7,\n",
    "                                  min_samples_split=10)\n",
    "#or new_model = gs.best_estimator_\n",
    "new_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(new_model, \"model.pkl\") #rename as per project name\n",
    "prediction = new_model.predict(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
